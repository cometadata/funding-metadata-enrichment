### Funding Metadata Extraction from Full Text Pipeline

This pipeline extracts funding acknowledgements from PDF documents, converting them into structured metadata linking them to their DOIs.

Optional preprocessing: If PDFs contain marginal line numbers or repeating headers that interfere with extraction,  [preprocess_pdfs utility](https://github.com/cometadata/funding-metadata-enrichment/tree/main/extract_funding_from_full_text/preprocess_pdfs) can be used to remove them before proceeding.

The extraction begins with the [batch_convert_pdfs utility](https://github.com/cometadata/funding-metadata-enrichment/tree/main/extract_funding_from_full_text/batch_convert_pdfs). This tool processes PDF files using [Docling](https://github.com/docling-project/docling), converting them to multiple formats, including markdown.

Once the PDFs are converted to markdown, [extract_funding_w_reranker utility](https://github.com/cometadata/funding-metadata-enrichment/tree/main/extract_funding_from_full_text/extract_funding_w_reranker) is used to extract the funding statements. Here, we employ a two-stage approach to identify and extract:

1. First we use a ColBERT model ([`GTE-ModernColBERT-v1`](https://huggingface.co/lightonai/GTE-ModernColBERT-v1)) that implements late interaction retrieval. Using this method, each document is broken down into a set of text sections. Then, instead of compressing an entire section into a single vector, we generate multiple contextualized embeddings for each of its tokens, creating a detailed, token-level matrix that represents the section's semantic content. When a query is then introduced by way of the config file, containing a set of funding statement related terms, it undergoes the same token-wise encoding. A ranking procedure then occurs using a token-to-token comparison. For each query token's embedding,  we scan all token embeddings within a text section to find those with the highest similarity. These individual maximum similarity scores are then summed to produce a single relevance score for the entire section. We do these calculations for every text section, effectively reranking the document's contents based on their semantic relevance to the queries, allowing us to identify those sections containing funding-related concepts. The process concludes by selecting only the top-k ranked text sections for passing on to the next stage.

2. The high-scoring text sections returned by the search undergo regex pattern matching to confirm they contain specific funding language text such as "awarded by", "funded by", "grant number", or "supported by". This (ideally) ensures that we filter out matches that may be semantically similar to funding statements, but that don't actually contain funding information. Only texts that achieve both a high similarity score and match at least one funding pattern are classed as funding statements.


Finally, we use [convert_normalize_ranking_extraction_json utility](https://github.com/cometadata/funding-metadata-enrichment/tree/main/extract_funding_from_full_text/convert_normalize_ranking_extraction_json) to post-processes the raw extraction output. This script maps funding statements to DOIs by parsing and comparing them against the filenames, as well as optionally normalizing the text of the funding statements. We reconcile the filenames with DOIs using a reference CSV file that contains all those used to create the input markdown files and then separate questionable entries (primarily those looking to contain failed text from the markdown conversion stage) into a separate output file.
