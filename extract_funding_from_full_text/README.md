### Funding Metadata Extraction from Full Text Pipeline

This pipeline extracts funding acknowledgements from PDF documents, converting them into structured metadata linking them to their DOIs.

The extraction begins with the [batch_convert_pdfs utility](https://github.com/cometadata/funding-metadata-enrichment/tree/main/extract_funding_from_full_text/batch_convert_pdfs). This tool processes PDF files using [Docling](https://github.com/docling-project/docling), converting them to multiple formats, including markdown.

Once the PDFs are converted to markdown, [extract_funding_w_reranker utility](https://github.com/cometadata/funding-metadata-enrichment/tree/main/extract_funding_from_full_text/extract_funding_w_reranker) is used to extract the funding statements. Here, we employ a two-stage approach to identify and extract:

1. First we use a ColBERT model ([`GTE-ModernColBERT-v1`](https://huggingface.co/lightonai/GTE-ModernColBERT-v1)) that implements late interaction retrieval. Unlike traditional dense retrieval methods that compress entire documents and queries into single vectors, ColBERT models generate multiple contextualized embeddings for each token in both the query and document. During search, the model computes relevance by finding the maximum similarity between each query token embedding and all document token embeddings, then sums these maximum similarities. This token-level matching allows the model to identify sections of text containing funding-related concepts even when expressed using varied phrasing. For example, it can recognize that "This work was made possible through support from..." corresponds to a funding acknowledgements even in the abscence of keyword matches.

2. High-scoring text sections returned by the model search then undergo regex pattern matching to confirm they contain specific funding language text such as "awarded by", "funded by", "grant number", or "supported by". This (ideally) ensures that we filter out matches that may be semantically similar to funding statements, but that don't actually contain funding information. Only texts that achieve both a high similarity score and match at least one funding pattern are classed as funding statements.


Finally, we use [convert_normalize_ranking_extraction_json utility](https://github.com/cometadata/funding-metadata-enrichment/tree/main/extract_funding_from_full_text/convert_normalize_ranking_extraction_json) to post-processes the raw extraction output. This script maps funding statements to DOIs by parsing and comparing them against the filenames, as well as optionally normalizing the text of the funding statements. We reconcile the filenames with DOIs using a reference CSV file that contains all those used to create the input markdown fules and then separate questionable entries (primarily those looking to contain failed text from the markdown conversion stage) into a separate output file.
